{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f86cac6",
   "metadata": {},
   "source": [
    "# <span style=\"background-color:#FFD700;  font-weight:bold; color:black;\"> Tutorial: Tradu√ß√£o de Textos com LangChain e Groq üöÄ</span>\n",
    "Este notebook demonstra como utilizar a biblioteca LangChain, em conjunto com o provedor Groq, para realizar tradu√ß√µes autom√°ticas de textos utilizando modelos de linguagem avan√ßados (como Llama 3).\n",
    "\n",
    "Voc√™ aprender√° a:\n",
    "- Instalar e configurar o ambiente necess√°rio;\n",
    "- Utilizar prompts din√¢micos para tradu√ß√£o;\n",
    "- Enviar mensagens para o modelo e interpretar as respostas.\n",
    "\n",
    "Cada c√©lula de c√≥digo cont√©m coment√°rios explicativos para facilitar o entendimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d30bfed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala o pacote langchain com suporte ao provedor Groq.\n",
    "# Necess√°rio para utilizar modelos de linguagem hospedados na Groq, como o Llama 3.\n",
    "\n",
    "# pip install -qU \"langchain[groq]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0f9db1",
   "metadata": {},
   "source": [
    "## Instala√ß√£o dos Pacotes Necess√°rios\n",
    "Antes de come√ßar, √© preciso instalar o pacote `langchain[groq]`, que permite a integra√ß√£o com modelos de linguagem hospedados na Groq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c16219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa bibliotecas para manipula√ß√£o de vari√°veis de ambiente e entrada de usu√°rio\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Solicita a chave da API Groq caso ainda n√£o esteja definida na vari√°vel de ambiente.\n",
    "# Isso √© necess√°rio para autenticar e acessar os modelos da Groq.\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "    os.environ[\"GROQ_API_KEY\"]= getpass.getpass(\"Entre com a chave da API Groq: \")\n",
    "\n",
    "# Importa fun√ß√£o para inicializar o modelo de chat da LangChain\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Inicializa o modelo Llama 3 hospedado na Groq para uso em tarefas de linguagem natural.\n",
    "model = init_chat_model(\"llama3-8b-8192\",model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c04ad0",
   "metadata": {},
   "source": [
    "## Configura√ß√£o da Chave de API e do Modelo\n",
    "Para utilizar os modelos da Groq, √© necess√°rio informar sua chave de API. Em seguida, inicializamos o modelo Llama 3, que ser√° usado para as tradu√ß√µes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f730214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¬°Hola, ¬øc√≥mo est√°s?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 35, 'total_tokens': 45, 'completion_time': 0.015540889, 'prompt_time': 0.005678618, 'queue_time': 0.144710301, 'total_time': 0.021219507}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_0fb809dba3', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--3f9a8abc-b39f-4ce3-af27-48551dccfe27-0', usage_metadata={'input_tokens': 35, 'output_tokens': 10, 'total_tokens': 45})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importa classes para criar mensagens do sistema e do usu√°rio (humano)\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Cria uma lista de mensagens para o modelo:\n",
    "# - Mensagem do sistema define a tarefa (traduzir para espanhol)\n",
    "# - Mensagem do humano cont√©m o texto a ser traduzido\n",
    "messages = [\n",
    "    SystemMessage(\"Traduza as palavras a seguir para o espanhol:\"),\n",
    "    HumanMessage(\"Ol√°, como voc√™ est√°?\"),\n",
    "# A lista pode ser expandida com mais mensagens se necess√°rio\n",
    "]\n",
    "\n",
    "# Envia as mensagens para o modelo e retorna a resposta traduzida\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbd1681",
   "metadata": {},
   "source": [
    "## Criando Mensagens para Tradu√ß√£o\n",
    "Agora vamos criar as mensagens que ser√£o enviadas ao modelo. A mensagem do sistema define a tarefa (traduzir para um idioma espec√≠fico) e a mensagem do humano cont√©m o texto a ser traduzido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0fae1021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|¬°|Hola|!| ¬ø|C√≥mo| est|√°s|?||"
     ]
    }
   ],
   "source": [
    "# Exemplo de streaming: recebe a resposta do modelo token por token.\n",
    "# √ötil para visualizar a resposta em tempo real, especialmente para respostas longas.\n",
    "for token in model.stream(messages):\n",
    "    print(token.content, end=\"|\")  # Imprime cada token seguido de '|'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec337bd",
   "metadata": {},
   "source": [
    "## Streaming da Resposta do Modelo\n",
    "Tamb√©m √© poss√≠vel receber a resposta do modelo em tempo real, token por token. Isso √© √∫til para acompanhar respostas longas √† medida que s√£o geradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "99844b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Traduza as palavras a seguir para o franc√™s', additional_kwargs={}, response_metadata={}), HumanMessage(content='Ol√°, como voc√™ est√°?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importa a classe para criar prompts din√¢micos para chat com modelos de linguagem\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define o template do sistema, permitindo parametrizar o idioma de destino na tradu√ß√£o.\n",
    "system_template = \"Traduza as palavras a seguir para o {idioma}\"\n",
    "\n",
    "# Cria o template do prompt, combinando mensagens do sistema e do usu√°rio.\n",
    "# Isso permite reutilizar a estrutura para diferentes idiomas e textos.\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),  # Mensagem do sistema com template para idioma\n",
    "    (\"human\", \"{texto}\"),        # Mensagem do humano com o texto a ser traduzido\n",
    "])\n",
    "\n",
    "# Gera o prompt preenchendo os par√¢metros: idioma e texto desejados.\n",
    "prompt = prompt_template.invoke({\"idioma\": \"franc√™s\", \"texto\": \"Ol√°, como voc√™ est√°?\"})\n",
    "prompt  # Exibe o prompt gerado para confer√™ncia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6c416",
   "metadata": {},
   "source": [
    "## Criando Prompts Din√¢micos para Tradu√ß√£o\n",
    "Utilizando o `ChatPromptTemplate`, √© poss√≠vel criar prompts din√¢micos e reutiliz√°veis, facilitando a tradu√ß√£o para diferentes idiomas e textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89828948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Traduza as palavras a seguir para o franc√™s', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Ol√°, como voc√™ est√°?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importa a classe para criar prompts din√¢micos para chat\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define o template do sistema para tradu√ß√£o parametrizada\n",
    "system_template = \"Traduza as palavras a seguir para o {idioma}\"\n",
    "\n",
    "# Cria o template do prompt com mensagens do sistema e do usu√°rio\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", \"{texto}\"),\n",
    "])\n",
    "\n",
    "# Gera o prompt preenchendo os par√¢metros\n",
    "prompt = prompt_template.invoke({\"idioma\": \"franc√™s\", \"texto\": \"Ol√°, como voc√™ est√°?\"})\n",
    "prompt  # Exibe o prompt gerado\n",
    "prompt.to_messages()  # Mostra as mensagens formatadas para o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef277c",
   "metadata": {},
   "source": [
    "## Visualizando as Mensagens do Prompt\n",
    "√â poss√≠vel visualizar como as mensagens s√£o formatadas antes de serem enviadas ao modelo, o que ajuda a entender a estrutura do prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "05af6eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, comment vas-tu?\n"
     ]
    }
   ],
   "source": [
    "# Importa a classe para criar prompts din√¢micos para chat\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define o template do sistema para tradu√ß√£o parametrizada\n",
    "system_template = \"Traduza as palavras a seguir para o {idioma}\"\n",
    "\n",
    "# Cria o template do prompt com mensagens do sistema e do usu√°rio\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", \"{texto}\"),\n",
    "])\n",
    "\n",
    "# Gera o prompt preenchendo os par√¢metros\n",
    "prompt = prompt_template.invoke({\"idioma\": \"franc√™s\", \"texto\": \"Ol√°, como voc√™ est√°?\"})\n",
    "prompt  # Exibe o prompt gerado\n",
    "prompt.to_messages()  # Mostra as mensagens formatadas para o modelo\n",
    "\n",
    "# Envia o prompt para o modelo e obt√©m a resposta traduzida\n",
    "resposta = model.invoke(prompt) \n",
    "print(resposta.content)  # Exibe o texto traduzido retornado pelo modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b8d490",
   "metadata": {},
   "source": [
    "## Executando o Prompt e Obtendo a Tradu√ß√£o\n",
    "Por fim, enviamos o prompt para o modelo e exibimos a resposta traduzida. Assim, √© poss√≠vel automatizar tradu√ß√µes para qualquer idioma suportado pelo modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdbcc16",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "186ead07",
   "metadata": {},
   "source": [
    "https://smith.langchain.com/o/318fe8b5-94f9-4877-ae33-810743e6904a/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
