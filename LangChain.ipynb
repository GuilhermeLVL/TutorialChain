{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f86cac6",
   "metadata": {},
   "source": [
    "# Tutorial: Tradução de Textos com LangChain e Groq\n",
    "Este notebook demonstra como utilizar a biblioteca LangChain, em conjunto com o provedor Groq, para realizar traduções automáticas de textos utilizando modelos de linguagem avançados (como Llama 3).\n",
    "\n",
    "Você aprenderá a:\n",
    "- Instalar e configurar o ambiente necessário;\n",
    "- Utilizar prompts dinâmicos para tradução;\n",
    "- Enviar mensagens para o modelo e interpretar as respostas.\n",
    "\n",
    "Cada célula de código contém comentários explicativos para facilitar o entendimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30bfed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Instala o pacote langchain com suporte ao provedor Groq.\n",
    "# Necessário para utilizar modelos de linguagem hospedados na Groq, como o Llama 3.\n",
    "pip install -qU \"langchain[groq]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0f9db1",
   "metadata": {},
   "source": [
    "## Instalação dos Pacotes Necessários\n",
    "Antes de começar, é preciso instalar o pacote `langchain[groq]`, que permite a integração com modelos de linguagem hospedados na Groq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c16219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa bibliotecas para manipulação de variáveis de ambiente e entrada de usuário\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "# Solicita a chave da API Groq caso ainda não esteja definida na variável de ambiente.\n",
    "# Isso é necessário para autenticar e acessar os modelos da Groq.\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "    os.environ[\"GROQ_API_KEY\"]= getpass.getpass(\"Entre com a chave da API Groq: \")\n",
    "\n",
    "# Importa função para inicializar o modelo de chat da LangChain\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Inicializa o modelo Llama 3 hospedado na Groq para uso em tarefas de linguagem natural.\n",
    "model = init_chat_model(\"llama3-8b-8192\",model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c04ad0",
   "metadata": {},
   "source": [
    "## Configuração da Chave de API e do Modelo\n",
    "Para utilizar os modelos da Groq, é necessário informar sua chave de API. Em seguida, inicializamos o modelo Llama 3, que será usado para as traduções."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f730214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¡Hola! ¿Cómo estás?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 35, 'total_tokens': 44, 'completion_time': 0.007350129, 'prompt_time': 0.004475604, 'queue_time': 0.14597242500000002, 'total_time': 0.011825733}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_5b339000ab', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--03f55d84-ca0d-4ddf-b56c-8d3caeb125a8-0', usage_metadata={'input_tokens': 35, 'output_tokens': 9, 'total_tokens': 44})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importa classes para criar mensagens do sistema e do usuário (humano)\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Cria uma lista de mensagens para o modelo:\n",
    "# - Mensagem do sistema define a tarefa (traduzir para espanhol)\n",
    "# - Mensagem do humano contém o texto a ser traduzido\n",
    "messages = [\n",
    "    SystemMessage(\"Traduza as palavras a seguir para o espanhol:\"),\n",
    "    HumanMessage(\"Olá, como você está?\"),\n",
    "# A lista pode ser expandida com mais mensagens se necessário\n",
    "]\n",
    "\n",
    "# Envia as mensagens para o modelo e retorna a resposta traduzida\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbd1681",
   "metadata": {},
   "source": [
    "## Criando Mensagens para Tradução\n",
    "Agora vamos criar as mensagens que serão enviadas ao modelo. A mensagem do sistema define a tarefa (traduzir para um idioma específico) e a mensagem do humano contém o texto a ser traduzido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fae1021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|¡|Hola|!| ¿|Cómo| est|ás|?||"
     ]
    }
   ],
   "source": [
    "# Exemplo de streaming: recebe a resposta do modelo token por token.\n",
    "# Útil para visualizar a resposta em tempo real, especialmente para respostas longas.\n",
    "for token in model.stream(messages):\n",
    "    print(token.content, end=\"|\")  # Imprime cada token seguido de '|'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec337bd",
   "metadata": {},
   "source": [
    "## Streaming da Resposta do Modelo\n",
    "Também é possível receber a resposta do modelo em tempo real, token por token. Isso é útil para acompanhar respostas longas à medida que são geradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99844b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Traduza as palavras a seguir para o francês', additional_kwargs={}, response_metadata={}), HumanMessage(content='Olá, como você está?', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importa a classe para criar prompts dinâmicos para chat com modelos de linguagem\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define o template do sistema, permitindo parametrizar o idioma de destino na tradução.\n",
    "system_template = \"Traduza as palavras a seguir para o {idioma}\"\n",
    "\n",
    "# Cria o template do prompt, combinando mensagens do sistema e do usuário.\n",
    "# Isso permite reutilizar a estrutura para diferentes idiomas e textos.\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),  # Mensagem do sistema com template para idioma\n",
    "    (\"human\", \"{texto}\"),        # Mensagem do humano com o texto a ser traduzido\n",
    "])\n",
    "\n",
    "# Gera o prompt preenchendo os parâmetros: idioma e texto desejados.\n",
    "prompt = prompt_template.invoke({\"idioma\": \"francês\", \"texto\": \"Olá, como você está?\"})\n",
    "prompt  # Exibe o prompt gerado para conferência"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6c416",
   "metadata": {},
   "source": [
    "## Criando Prompts Dinâmicos para Tradução\n",
    "Utilizando o `ChatPromptTemplate`, é possível criar prompts dinâmicos e reutilizáveis, facilitando a tradução para diferentes idiomas e textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89828948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Traduza as palavras a seguir para o francês', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Olá, como você está?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importa a classe para criar prompts dinâmicos para chat\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define o template do sistema para tradução parametrizada\n",
    "system_template = \"Traduza as palavras a seguir para o {idioma}\"\n",
    "\n",
    "# Cria o template do prompt com mensagens do sistema e do usuário\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", \"{texto}\"),\n",
    "])\n",
    "\n",
    "# Gera o prompt preenchendo os parâmetros\n",
    "prompt = prompt_template.invoke({\"idioma\": \"francês\", \"texto\": \"Olá, como você está?\"})\n",
    "prompt  # Exibe o prompt gerado\n",
    "prompt.to_messages()  # Mostra as mensagens formatadas para o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef277c",
   "metadata": {},
   "source": [
    "## Visualizando as Mensagens do Prompt\n",
    "É possível visualizar como as mensagens são formatadas antes de serem enviadas ao modelo, o que ajuda a entender a estrutura do prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05af6eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bonjour, comment vas-tu?\n"
     ]
    }
   ],
   "source": [
    "# Importa a classe para criar prompts dinâmicos para chat\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define o template do sistema para tradução parametrizada\n",
    "system_template = \"Traduza as palavras a seguir para o {idioma}\"\n",
    "\n",
    "# Cria o template do prompt com mensagens do sistema e do usuário\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"human\", \"{texto}\"),\n",
    "])\n",
    "\n",
    "# Gera o prompt preenchendo os parâmetros\n",
    "prompt = prompt_template.invoke({\"idioma\": \"francês\", \"texto\": \"Olá, como você está?\"})\n",
    "prompt  # Exibe o prompt gerado\n",
    "prompt.to_messages()  # Mostra as mensagens formatadas para o modelo\n",
    "\n",
    "# Envia o prompt para o modelo e obtém a resposta traduzida\n",
    "resposta = model.invoke(prompt) \n",
    "print(resposta.content)  # Exibe o texto traduzido retornado pelo modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b8d490",
   "metadata": {},
   "source": [
    "## Executando o Prompt e Obtendo a Tradução\n",
    "Por fim, enviamos o prompt para o modelo e exibimos a resposta traduzida. Assim, é possível automatizar traduções para qualquer idioma suportado pelo modelo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
