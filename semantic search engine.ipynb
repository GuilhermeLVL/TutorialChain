{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de85e1c",
   "metadata": {},
   "source": [
    "# Tutorial: Buscador Semântico de PDFs\n",
    "\n",
    "Este notebook mostra como criar um buscador inteligente que entende o significado dos textos em arquivos PDF. Você poderá pesquisar por assuntos, não só por palavras exatas.\n",
    "\n",
    "**Exemplo:** Se você procurar por \"energia renovável\", o sistema pode encontrar textos que falam sobre \"painéis solares\" ou \"energia eólica\", mesmo que não usem exatamente as mesmas palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524d1a1",
   "metadata": {},
   "source": [
    "## 1. Instalação dos Pacotes\n",
    "\n",
    "**O que está acontecendo:**\n",
    "Instalamos as bibliotecas necessárias para manipular PDFs, criar representações numéricas dos textos (embeddings), dividir textos em partes menores e construir o banco de dados para buscas.\n",
    "\n",
    "**Por quê:**\n",
    "Essas bibliotecas são a base para todo o pipeline de busca semântica.\n",
    "\n",
    "**Exemplo:**\n",
    "Imagine que cada frase do PDF vira uma lista de números. Assim, o computador consegue comparar o significado dos textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eb68389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: pypdf in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (5.9.0)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (1.11.0.post1)\n",
      "Requirement already satisfied: langchain-huggingface in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langchain) (0.3.72)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langchain) (0.3.9)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langchain) (0.4.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langchain) (2.11.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langchain) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langchain) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langchain-community) (3.12.15)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langchain-community) (2.3.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langchain-huggingface) (0.21.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.33.4 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langchain-huggingface) (0.34.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (2025.7.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from huggingface-hub>=0.33.4->langchain-huggingface) (4.14.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.11.1)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2025.7.14)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.33.4->langchain-huggingface) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Instale as bibliotecas necessárias (execute apenas uma vez)\n",
    "!pip install langchain langchain-community pypdf faiss-cpu langchain-huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916631b6",
   "metadata": {},
   "source": [
    "## 2. Importação das Bibliotecas\n",
    "\n",
    "**O que está acontecendo:**\n",
    "Aqui importamos as ferramentas que vão ler os PDFs, dividir os textos, criar os vetores e armazenar tudo para busca.\n",
    "\n",
    "**Por quê:**\n",
    "Cada biblioteca tem uma função específica no processo.\n",
    "\n",
    "**Exemplo:**\n",
    "- `os` serve para navegar pelas pastas do computador.\n",
    "- `PyPDFLoader` lê o conteúdo dos PDFs.\n",
    "- `RecursiveCharacterTextSplitter` quebra textos grandes em pedaços menores.\n",
    "- `HuggingFaceEmbeddings` transforma textos em vetores (listas de números).\n",
    "- `FAISS` é o banco de dados rápido para buscar por significado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80afc781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Importação das Bibliotecas\n",
    "# O que está acontecendo:\n",
    "# Aqui importamos as bibliotecas essenciais para o pipeline de busca semântica.\n",
    "# - os: manipulação de arquivos e diretórios\n",
    "# - PyPDFLoader: carrega PDFs e transforma em documentos\n",
    "# - RecursiveCharacterTextSplitter: divide textos em pedaços menores (chunks)\n",
    "# - HuggingFaceEmbeddings: gera vetores semânticos dos textos\n",
    "# - FAISS: banco vetorial para buscas rápidas\n",
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d1654b",
   "metadata": {},
   "source": [
    "## 3. Carregando os PDFs\n",
    "\n",
    "**O que está acontecendo:**\n",
    "O código abaixo lê todos os arquivos PDF de uma pasta e junta todas as páginas em uma lista.\n",
    "\n",
    "**Por quê:**\n",
    "Assim, conseguimos trabalhar com todos os textos dos PDFs de uma vez.\n",
    "\n",
    "**Exemplo:**\n",
    "Se você tem 3 PDFs com 10 páginas cada, o sistema vai juntar as 30 páginas em uma lista para processar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f939251f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de arquivos PDF encontrados: 7\n",
      "Total de páginas carregadas: 722\n"
     ]
    }
   ],
   "source": [
    "# Defina o diretório onde os PDFs enviados pelo usuário estão salvos\n",
    "pdf_dir = r\"C:\\Users\\Guilh\\OneDrive\\Área de Trabalho\\Projetos IA\\TutorialChain\\Docs\\Artigos\"   # Altere para o nome da sua pasta de uploads\n",
    "\n",
    "# Lista para armazenar todos os documentos carregados\n",
    "docs = []\n",
    "\n",
    "# Percorre todos os arquivos da pasta e carrega os PDFs\n",
    "for filename in os.listdir(pdf_dir):\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(os.path.join(pdf_dir, filename))\n",
    "        docs.extend(loader.load())\n",
    "\n",
    "print(f\"Total de arquivos PDF encontrados: {len([f for f in os.listdir(pdf_dir) if f.lower().endswith('.pdf')])}\")\n",
    "print(f\"Total de páginas carregadas: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e25c2a",
   "metadata": {},
   "source": [
    "## 4. Dividindo o Texto em Partes Menores (Chunks)\n",
    "\n",
    "**O que está acontecendo:**\n",
    "Os textos dos PDFs são divididos em pedaços menores, chamados de \"chunks\".\n",
    "\n",
    "**Por quê:**\n",
    "Buscar em textos menores é mais eficiente e preciso. Assim, o sistema pode mostrar só o trecho relevante.\n",
    "\n",
    "**Exemplo:**\n",
    "Se uma página tem 3.000 caracteres, ela pode ser dividida em 3 pedaços de 1.000 caracteres cada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bf80fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks gerados: 2877\n"
     ]
    }
   ],
   "source": [
    "# Divide os textos em chunks menores para facilitar a busca semântica\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Total de chunks gerados: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c607dbde",
   "metadata": {},
   "source": [
    "## 5. Criando os Vetores (Embeddings)\n",
    "\n",
    "**O que está acontecendo:**\n",
    "Cada pedaço de texto é transformado em um vetor (lista de números) que representa o significado daquele texto.\n",
    "\n",
    "**Por quê:**\n",
    "O computador não entende palavras, mas entende números. Assim, conseguimos comparar o significado dos textos.\n",
    "\n",
    "**Exemplo:**\n",
    "A frase \"gato preto\" pode virar o vetor `[0.12, -0.34, 0.56, ...]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba842396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Guilh\\OneDrive\\Área de Trabalho\\Projetos IA\\TutorialChain\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cria o objeto de embeddings usando um modelo HuggingFace (especializado em transformar texto em vetores)\n",
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7682d4",
   "metadata": {},
   "source": [
    "## 6. Criando e Salvando o Banco Vetorial\n",
    "\n",
    "**O que está acontecendo:**\n",
    "Os vetores dos textos são armazenados em um banco de dados especial (FAISS), que permite buscas rápidas por significado.\n",
    "\n",
    "**Por quê:**\n",
    "Assim, quando você faz uma pergunta, o sistema encontra rapidamente os textos mais parecidos com o que você procurou.\n",
    "\n",
    "**Exemplo:**\n",
    "Se você perguntar \"Como funciona energia solar?\", o sistema busca os vetores mais próximos dessa pergunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "910827e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (1.11.0.post1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from faiss-cpu) (2.3.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\guilh\\onedrive\\área de trabalho\\projetos ia\\tutorialchain\\.venv\\lib\\site-packages (from faiss-cpu) (25.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Antes de usar FAISS, é necessário instalar o pacote faiss-cpu (ou faiss-gpu se tiver GPU compatível).\n",
    "# Execute a linha abaixo se ainda não instalou:\n",
    "!pip install faiss-cpu\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Cria o banco vetorial com os embeddings dos chunks\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "\n",
    "# Salve o banco vetorial FAISS após criar para evitar reprocessamento\n",
    "vector_store.save_local(\"meu_banco_vetorial\")  # Salva o banco vetorial em disco"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfc8aea",
   "metadata": {},
   "source": [
    "## 7. Exibindo os Resultados da Busca\n",
    "\n",
    "**O que está acontecendo:**\n",
    "Aqui você pode filtrar e visualizar os resultados encontrados pelo buscador semântico.\n",
    "\n",
    "**Por quê:**\n",
    "Assim, você vê o título, página e um trecho do texto encontrado, facilitando a leitura.\n",
    "\n",
    "**Exemplo:**\n",
    "Se você buscar por \"inteligência artificial\", o sistema mostra o título do artigo, a página e um pedaço do texto onde o assunto aparece.\n",
    "\n",
    "---\n",
    "\n",
    "### Como fazer uma busca:\n",
    "1. Escreva sua pergunta ou termo de interesse na variável `consulta`.\n",
    "2. O sistema irá procurar os textos mais parecidos com sua pergunta.\n",
    "\n",
    "**Exemplo de consulta:**\n",
    "- \"O que é aprendizado de máquina?\"\n",
    "- \"Exemplos de energia renovável\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "489f4985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faça uma busca semântica no banco vetorial\n",
    "consulta = \"O que é inteligência artificial?\"  # Altere para sua pergunta\n",
    "resultados_busca = vector_store.similarity_search(consulta, k=5)  # Retorna os 5 textos mais próximos\n",
    "\n",
    "# Salva os resultados na variável 'results' para exibição\n",
    "results = resultados_busca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "012bb751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Resultado 1 ---\n",
      "Título: \n",
      "Página: 296\n",
      "Trecho:\n",
      "CHAPTER 14 Question Answering, Informa- tion Retrieval, and Retrieval- Augmented Generation People need to know things. So pretty much as soon as there were computers we were asking them questions. Systems in the 1960s were answering questions about baseball statistics and scientiﬁc facts. Even ﬁctional computers in the 1970s like Deep Thought, invented by Douglas Adams inThe Hitchhiker’s Guide to the Galaxy, answered “the Ultimate Question Of Life, The Universe, and Everything”. 1 And because s\n",
      "------------------------------------------------------------\n",
      "--- Resultado 2 ---\n",
      "Título: \n",
      "Página: 335\n",
      "Trecho:\n",
      "(Levesque et al., 1990), and shared plans (Grosz and Sidner, 1980). The earliest conversational systems were simple pattern-action chatbots like ELIZA (Weizenbaum, 1966). ELIZA had a widespread inﬂuence on popular perceptions of artiﬁcial intelligence, and brought up some of the ﬁrst ethical questions in natural language processing —such as the issues of privacy we discussed above as well the role of algorithms in decision-making— leading its creator Joseph Weizenbaum to ﬁght for social responsi\n",
      "------------------------------------------------------------\n",
      "--- Resultado 3 ---\n",
      "Título: \n",
      "Página: 63\n",
      "Trecho:\n",
      "56 CHAPTER 4 • N AIVE BAYES , TEXT CLASSIFICATION , AND SENTIMENT CHAPTER 4 Naive Bayes, Text Classiﬁca- tion, and Sentiment Classiﬁcation lies at the heart of both human and machine intelligence. Deciding what letter, word, or image has been presented to our senses, recognizing faces or voices, sorting mail, assigning grades to homeworks; these are all examples of assigning a category to an input. The potential challenges of this task are highlighted by the fabulist Jorge Luis Borges (1964), wh\n",
      "------------------------------------------------------------\n",
      "--- Resultado 4 ---\n",
      "Título: \n",
      "Página: 333\n",
      "Trecho:\n",
      "system (Friedman et al. 2017, Friedman and Hendry 2019). 15.5.1 Ethical Issues in Dialogue System Design Ethical issues have been key to how we think about designing artiﬁcial agents since well before we had dialogue systems. Mary Shelley (depicted below) centered her novel Frankensteinaround the problem of creating artiﬁcial agents without consider- ing ethical and humanistic concerns. One issue is the safety of users. If users seek information from di- alogue systems in safety-critical situati\n",
      "------------------------------------------------------------\n",
      "--- Resultado 5 ---\n",
      "Título: \n",
      "Página: 508\n",
      "Trecho:\n",
      "ond one”. A question answering system that uses Wikipedia to answer a question about Marie Curie must know who she was in the sentence “She was born in War- saw”. And a machine translation system translating from a language like Spanish, in which pronouns can be dropped, must use coreference from the previous sentence to decide whether the Spanish sentence ‘“Me encanta el conocimiento”, dice. ’ should be translated as ‘“I love knowledge”, he says ’, or ‘“I love knowledge”, she says ’. Indeed, th\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "palavra_chave_titulo = \"\"   # Deixe vazio para mostrar todos, ou coloque uma palavra para filtrar\n",
    "\n",
    "for i, doc in enumerate(results, 1):\n",
    "    titulo = doc.metadata.get('title', '').lower()\n",
    "    trecho = doc.page_content[:500].replace('\\n', ' ')\n",
    "    if not palavra_chave_titulo or palavra_chave_titulo in titulo:\n",
    "        print(f'--- Resultado {i} ---')\n",
    "        print(f'Título: {doc.metadata.get(\"title\", \"Sem título\")}')\n",
    "        print(f'Página: {doc.metadata.get(\"page\", \"Desconhecida\")}')\n",
    "        print('Trecho:')\n",
    "        print(trecho)\n",
    "        print('-' * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
